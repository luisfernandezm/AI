# -*- coding: utf-8 -*-
"""webScrappingAplicado.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XcVVDChq6jI5yh_jfY62Dcn31MRvUqNx
"""

# Importar librerías
import requests
import pandas as pd
import numpy as np
from bs4 import BeautifulSoup
from tqdm import tqdm

# Definir dirección de URL
url = requests.get("https://www.pagina12.com.ar/")

# Revisar el status code de la URL
url.status_code

# Crear variable donde recibas el texto de la URL y realizar scrapping
scrapping = BeautifulSoup(url.text, "lxml")

# Imprimir el texto de la URL
print(url.text)

# Mostrar el código de la URL
url.content

# Mostrar encabezados de la URL
url.headers

# Realizar solicitud de scrapping desde Python
url.request.headers

# Método para realizar el request
url.request.method

# Obtener dirección de donde se realiza el request (GET)
url.request.url

# Tipo de la variable 'scrapping'
type(scrapping)

# imprimir el código html de la url
print(scrapping)

# Imprimir el html de la URL e identandolo con la función 'prettify()'
print(scrapping.prettify())

# Extraer el primer elemento, en este caso 'ul'
scrapping.find('ul')

# Encontrar todos los elementos de la página (en este caso todos los 'ul') que contengan el atributo indicado (en este caso la class)
scrapping.find('ul', attrs = {'class': 'horizontal-list main-sections hide-on-dropdown'})

# Encontrar todos los elementos indicados (en este caso todos los 'li')
elementos = scrapping.find('ul', attrs = {'class': 'horizontal-list main-sections hide-on-dropdown'}).find_all('li')

# Función para realizar el web scrapping
ulTag = scrapping.find('ul', attrs = {'class': 'horizontal-list main-sections hide-on-dropdown'})

if ulTag:
  liTag = ulTag.find_all('li')

  for li in liTag:
    aTag = li.find('a')

    if aTag:
      print(aTag.get('href'))

else:
  print('Elemento no encontrado')

# Definir url del articulo y realizar el GET request
urlArticulo = requests.get('https://www.pagina12.com.ar/589381-arsenal-de-sarandi-federico-vilar-dejo-de-ser-el-tecnico')

# Verificar que la solicitud fue exitosa
if urlArticulo.status_code == 200:

  # Guardar el texto de la url en la variable 'soupArticulo'
  soupArticulo = BeautifulSoup(urlArticulo.content, 'lxml')

  # Ir a la url del articulo y seleccionar el div del que se desea hacer scrapping
  articuloDiv = soupArticulo.find('div', class_ = 'section-2-col article-main-content')

  # Revisar que el articulo exista
  if articuloDiv:

    # Si el articulo existe, imprimir el texto
    print(articuloDiv.get_text())

  else:
    print('Articulo no encontrado')

else:
  print('URL invalido')

# Extraer todos las secciones de la url
elemento = elementos[4]
print(elemento)

linkSecciones = [elemento.a.get('href') for elemento in elementos]
linkSecciones

# Seleccionar el <a> de la variable elemento
elemento.a

# Obtener el texto dentro del <a> en la variable elemento
elemento.a.get_text()

# Revisar el status code de la url
seccion = requests.get(linkSecciones[4])
seccion.status_code

# Extraer el html de la sección
extraccionHTML = BeautifulSoup(seccion.text,'lxml')
print(extraccionHTML.prettify())

featuredArticle = extraccionHTML.find("div", attrs = {"class": "articles-list"})
featuredArticle

# Generar información de cada articulo
articulos = []

for articulo in featuredArticle.find_all('article', class_='article-item-featured'):

  # Extraer la información
  titulo = articulo.find('h3', class_= "title").a.text
  enlace = articulo.find('h3', class_= "title").a['href']
  resumen = articulo.find('p', class_= "title").a['href']
  fecha = articulo.find('div', class_= "date").a.text
  autorElement = articulo.find('div', class_= "author")
  autor = autorElement.a.text if autorElement else None

  # Almacenar información
  infoArticulo ={
      "Titulo": titulo,
      "Enlace": enlace,
      "Resumen": resumen,
      "Fecha": fecha,
      "Autor": autor
  }

  articulos.append(infoArticulo)
  print(infoArticulo)

#Imprimir la información
for articulo in articulos:
  for key, value in articulo.items():
    print(f"{key}: {value}")
  print("----------")



# Para imagenes
imagenes = extraccionHTML.find_all('img')

for imagen in imagenes:
  print(imagen["src"])

urlImg = "https://images.pagina12.com.ar/styles/focal_3_2_470x313/public/2023-09/770299-messi-20-40miguegranados.jpg?h=ada05aa9&itok=PpdL19M-"

imagenEncontrada = any(imagen['src'] == urlImg for imagen in imagenes)

if imagenEncontrada:
  print(f'URL: {urlImg}')
else:
  print('Imagen no encontrada')

from IPython.display import Image, display

#Mostrar imagen
display(Image(url = urlImg))